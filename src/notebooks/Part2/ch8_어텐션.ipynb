{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 어텐션의 구조\n",
    "\n",
    "- 어텐션 메커니즘, seq2seq는 필요한 정보에만 주목할 수 있게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.1 seq2seq의 문제점\n",
    "\n",
    "- '고정 길이'라는데에 큰 문제가 잠재해 있다.\n",
    "- 입력 문장의 길에 관계없이 항상 같은 길이의 벡터로 변환한다는 뜻\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-1.png\" height=\"200\">\n",
    "\n",
    "- 많은 옷가지를 옷장에 욱여넣듯이 억지로 고정 길이의 벡터로 밀어 넣는 꼴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.2 Encoder 개선\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-2.png\" height=\"400\">\n",
    "\n",
    "- 기존 : 마지막 은닉 상태만을 Decoder에 전달\n",
    "- Encoder 출력의 길이는 입력 문장의 길에 따라 바꿔주는게 좋다.\n",
    "- LSTM 계층의 은닉 상태 벡터를 모두 이용\n",
    "- 입력된 단어와 같은 수의 벡터를 얻을 수 있음\n",
    "- '하나의 고정 길이 벡터'라는 제약으로부터 해방됨\n",
    "- LSTM 계층의 은닉 상태의 '내용'\n",
    "  - 각 시각의 은닉 상태에는 직전에 입력된 단어에 대한 정보가 많이 포함되어 있다.\n",
    "  - $h_t$에는 $x_t$에 대한 정보가 많이 포함됨\n",
    "  - Encoder가 출력하는 hs행렬은 각 단어에 해당하는 벡터들의 집합 \n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-3.png\" height=\"300\">\n",
    "\n",
    "- 주변 정보를 균형있게 담아야 할 때, 시계열 데이터를 양방향으로 처리해야할 떄\n",
    "- 양방향 RNN or 양방향 LSTM이 효과적\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.3 Decoder 개선 1\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-4.png\" height=\"200\">\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-5.png\" height=\"200\">\n",
    "\n",
    "- hs 전부를 이용할 수 있도록 Decoder를 개선\n",
    "- 사람: 어떤 단어(혹은 단어의 집합)에 주목하여 그 단어의 변환을 수시로 하게 됨\n",
    "- 입력과 출력의 여러 단어 중 어떤 단어끼리 서로 관련되어 있는가 라는 대응 관계를 학습시킬 수 없을까?\n",
    "- alignment\n",
    "  - 고양이 = cat\n",
    "  - 단어의 대응관계를 나타내는 작업\n",
    "  - 수작업 -> 기계에 의한 자동화\n",
    "\n",
    "- 목표는 '도착어 단어'와 대응관계에 있는 '출발 단어'의 정보를 골라내는 것.\n",
    "- 이를 이용해 번역을 수행하는 것\n",
    "- 필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행하는 것이 목표 \n",
    "- 어텐션\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-6.png\" height=\"400\">\n",
    "\n",
    "- 어떤 계산을 수행하는 계층을 추가\n",
    "- 받는 입력은 2가지\n",
    "  - Encoder로부터 받는 hs\n",
    "  - 시각별 LSTM 계층의 은닉 상태\n",
    "- 이 중 필요한 정보만 골라 위쪽의 Affine 계층으로 출력\n",
    "- Encoder의 마지막 은닉 상태 벡터는 Decoder의 첫 번째 LSTM 계층에 전달\n",
    "\n",
    "- 단어들의 얼라인먼트 추출\n",
    "  - 각 시각에서 Decoder에 입력된 단어와 대응 관계인 단어의 벡터를 hs에서 골라내겠다.\n",
    "  - '선택'작업을 '어떤 계산'으로 해내겠다.\n",
    "\n",
    "- 문제 발생\n",
    "  - 선택하는 작업은 미분할 수 없다는 점 -> 오차역전파 사용 불가능\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-7.png\" height=\"200\">\n",
    "\n",
    "- 하나를 선택하는게 아니라 모든 것을 선택한다. 그리고 각 단어의 중요도를 나타내는 가중치를 별도로 계산\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-8.png\" height=\"200\">\n",
    "\n",
    "- 맥락 벡터 c에는 현 시각의 변환(번역)을 수행하는데 필요한 정보가 담겨있다.  \n",
    "(정확히는 그렇게 되도록 데이터로부터 학습한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중합 예시\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-9.png\" height=\"150\">\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-10.png\" height=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "(5, 4)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "# ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "ar = a.reshape(5, 1)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중합 계산 그래프\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-11.png\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.cache = None\n",
    "    \n",
    "  def forward(self, hs, a):\n",
    "    N, T, H = hs.shape\n",
    "    \n",
    "    ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "    t = hs * ar\n",
    "    c = np.sum(t, axis=1)\n",
    "    \n",
    "    self.cache = (hs, ar)\n",
    "    \n",
    "    return c\n",
    "  \n",
    "  def backward(self, dc):\n",
    "    hs, ar = self.cache\n",
    "    N, T, H = hs.shape\n",
    "    \n",
    "    dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "    dar = dt * hs\n",
    "    dhs = dt * ar\n",
    "    da = np.sum(dar, axis=2)\n",
    "    \n",
    "    return dhs, da\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.4 Decoder 개선 2\n",
    "\n",
    "- 가중치 a가 있다면 , 가중합을 이용해 '맥락 벡터' 얻을 수 있음\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-12.png\" height=\"400\">\n",
    "\n",
    "- h가 hs의 각 단어 벡터와 얼마나 '비슷한가'를 수치로 나타내는 것\n",
    "- 내적 이용 , 두 벡터가 얼마나 같은 방향을 향하고 있는가\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-13.png\" height=\"400\">\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-14.png\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../modules/Part2/')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h = np.random.randn(N, H)\n",
    "# hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "hr = h.reshape(N, 1, H)\n",
    "\n",
    "t = hs * hr\n",
    "\n",
    "s = np.sum(t, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치 계산 그래프\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-15.png\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../modules/Part2/')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "class AttentionWeight:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.softmax = Softmax()\n",
    "    self.cache = None\n",
    "    \n",
    "  def forward(self, hs, h):\n",
    "    N, T, H = hs.shape\n",
    "    \n",
    "    hr = h.reshape(N, 1, H)\n",
    "    t = hs * hr\n",
    "    s = np.sum(t, axis=2)\n",
    "    \n",
    "    a = self.softmax.forward(s)\n",
    "    \n",
    "    self.cache = (hs, hr)\n",
    "    \n",
    "    return a\n",
    "  \n",
    "  def backward(self, da):\n",
    "    hs, hr = self.cache\n",
    "    N, T, H = hs.shape\n",
    "    \n",
    "    ds = self.softmax.backward(da)\n",
    "    dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "    dhs = dt * hr\n",
    "    dhr = dt * hs\n",
    "    dh = np.sum(dhr, axis=1)\n",
    "    \n",
    "    return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.5 Decoder 개선 3\n",
    "\n",
    "- weight sum + attention weight\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-16.png\" height=\"300\">\n",
    "\n",
    "- Attention Weight , Encoder가 출력하는 각 단어의 벡터 hs에 주목하여 단어의 가중치 a를 구함\n",
    "- Weight Sum 계층이 a와 hs의 가중합을 구하고, 그 결과를 맥락 벡터 c로 출력\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-17.png\" height=\"300\">\n",
    "\n",
    "- Encoder가 건네주는 정보 hs에서 중요한 원소에 주목하여 그것을 바탕으로 맥락 벡터를 구해 위쪽 계층으로 전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.attention_weight_layer = AttentionWeight()\n",
    "    self.weight_sum_layer = WeightSum()\n",
    "    self.attention_weight = None\n",
    "    \n",
    "  def forward(self, hs, h): # 맥락 벡터 구함\n",
    "    a = self.attention_weight_layer.forward(hs, h)\n",
    "    out = self.weight_sum_layer.forward(hs, a)\n",
    "    self.attention_weight = a\n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "    dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "    dhs = dhs0 + dhs1\n",
    "    return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 구조\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-18.png\" height=\"500\">\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-19.png\" height=\"300\">\n",
    "\n",
    "- Decoder에 Attention 계층이 구한 맥락 벡터 정보를 '추가'한 것으로 생각할 수 있다.\n",
    "- Affine 계층에는 기존과 마찬가지로 LSTM 계층의 은닉 상태 벡터를 주고,   \n",
    "여기에 더해 Attention 계층의 맥락 벡터까지 입력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Attention으로 구축\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-20.png\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.layers = None\n",
    "    self.attention_weight = None\n",
    "    \n",
    "  def forward(self, hs_enc, hs_dec): # 맥락 벡터 구함\n",
    "    N, T, H = hs_dec.shape\n",
    "    out = np.empty_like(hs_dec)\n",
    "    self.layers = []\n",
    "    self.attention_weight = []\n",
    "    \n",
    "    for t in range(T):\n",
    "      layer = Attention()\n",
    "      out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "      self.layers.append(layer)\n",
    "      self.attention_weight.append(layer.attention_weight)\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    N, T, H = dout.shape\n",
    "    dhs_enc = 0\n",
    "    dhs_dec = np.empty_like(dout)\n",
    "    \n",
    "    for t in range(T):\n",
    "      layer = self.layers[t]\n",
    "      dhs, dh = layer.backward(dout[:, t, :])\n",
    "      dhs_enc += dhs\n",
    "      dhs_dec[:, t, :] = dh\n",
    "    \n",
    "    return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 어텐션을 갖춘 seq2seq 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.1 Encoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../modules/Part2/')\n",
    "from common.time_layers import *\n",
    "from seq2seq import Encoder, Seq2seq\n",
    "from attention_layer import TimeAttention\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "  def forward(self, xs):\n",
    "    xs = self.embed.forward(xs)\n",
    "    hs = self.lstm.forward(xs)\n",
    "    return hs\n",
    "  \n",
    "  def backward(self, dhs):\n",
    "    dout = self.lstm.backward(dhs)\n",
    "    dout = self.embed.backward(dout)\n",
    "    return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.2 Decoder 구현\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-21.png\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "    rn = np.random.randn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.3 seq2seq 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../modules/Part2/')\n",
    "\n",
    "from seq2seq import Encoder, Seq2seq\n",
    "\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "    args = vocab_size, wordvec_size, hidden_size\n",
    "    self.encoder = AttentionEncoder(*args)\n",
    "    self.decoder = AttentionDecoder(*args)\n",
    "    self.softmax = TimeSoftmaxWithLoss()\n",
    "    \n",
    "    self.params = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 어텐션 평가\n",
    "\n",
    "- 번역용 데이터셋 중에는 'WMT'가 유명. \n",
    "  - 영어와 프랑스어 학습 데이터가 쌍으로 준비되어 있다.\n",
    "  - 많은 연구에서 벤치마크로 이용됨\n",
    "  - 덩치가 커서 부담됨 ( 20GB )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.1 날짜 형식 변환 문제\n",
    "\n",
    "- 영어권에서 사용되는 다양한 날짜 형식을 표준 형식으로 변환하는 것이 목표\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-22.png\" height=\"200\">\n",
    "\n",
    "- 겉보기만큼 간단하지 않다.\n",
    "  - 다양한 변형, 다양한 변환 규칙. \n",
    "- 입력과 출력 사이에 알기 쉬운 대응 관계가 있다.\n",
    "  - 년-월-일의 대응관계가 존재\n",
    "  - 어텐션이 각각의 원소에 올바르게 주목하고 있는지를 확인 가능\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-23.png\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.2 어텐션을 갖춘 seq2seq의 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../modules/Part2/')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "from seq2seq import Seq2seq\n",
    "from peeky_seq2seq import PeekySeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/7/72                        _1972-02-07\n",
      "Apr 6, 1993                   _1993-04-06\n",
      "December 13, 1999             _1999-12-13\n",
      "monday, january 29, 2001      _2001-01-29\n",
      "Tuesday, October 19, 1976     _1976-10-19\n",
      "May 1, 1972                   _1972-05-01\n",
      "12/3/87                       _1987-12-03\n",
      "8/2/78                        _1978-08-02\n",
      "June 25, 2006                 _2006-06-25\n",
      "Jul 3, 2000                   _2000-07-03\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print(''.join([id_to_char[a] for a in x_train[i]]), end=' ')\n",
    "  print(''.join([id_to_char[a] for a in t_train[i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 6[s] | 손실 3.08\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 12[s] | 손실 1.90\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 19[s] | 손실 1.72\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 25[s] | 손실 1.47\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 31[s] | 손실 1.19\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 38[s] | 손실 1.14\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 44[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 50[s] | 손실 1.06\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 57[s] | 손실 1.05\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 63[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 69[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 76[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 83[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 89[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 96[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 103[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 109[s] | 손실 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1977-07-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1977-07-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1977-07-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1977-07-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1977-07-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1977-07-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1977-07-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1977-07-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1977-07-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1977-07-11\n",
      "---\n",
      "검증 정확도 : 0.000\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 7[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 14[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 21[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 28[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 34[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 41[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 48[s] | 손실 0.97\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 55[s] | 손실 0.96\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 61[s] | 손실 0.93\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 68[s] | 손실 0.90\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 74[s] | 손실 0.86\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 81[s] | 손실 0.80\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 87[s] | 손실 0.74\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 94[s] | 손실 0.67\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 100[s] | 손실 0.60\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 107[s] | 손실 0.50\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 113[s] | 손실 0.38\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1972-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 2013-12-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "검증 정확도 : 58.400\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 7[s] | 손실 0.23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m acc_list \u001b[39m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epoch):\n\u001b[1;32m----> 3\u001b[0m   trainer\u001b[39m.\u001b[39;49mfit(x_train, t_train, max_epoch\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size, max_grad\u001b[39m=\u001b[39;49mmax_grad)\n\u001b[0;32m      5\u001b[0m   correct_num \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      6\u001b[0m   \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(x_test)):\n",
      "File \u001b[1;32mc:\\Users\\Hi\\Desktop\\DeepLearningfromScratch\\src\\notebooks\\Part2\\../../modules/Part2\\common\\trainer.py:41\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m# 기울기 구해 매개변수 갱신\u001b[39;00m\n\u001b[0;32m     40\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(batch_x, batch_t)\n\u001b[1;32m---> 41\u001b[0m model\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     42\u001b[0m params, grads \u001b[39m=\u001b[39m remove_duplicate(model\u001b[39m.\u001b[39mparams, model\u001b[39m.\u001b[39mgrads)  \u001b[39m# 공유된 가중치를 하나로 모음\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m max_grad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hi\\Desktop\\DeepLearningfromScratch\\src\\notebooks\\Part2\\../../modules/Part2\\seq2seq.py:114\u001b[0m, in \u001b[0;36mSeq2seq.backward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m    112\u001b[0m dout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax\u001b[39m.\u001b[39mbackward(dout)\n\u001b[0;32m    113\u001b[0m dh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mbackward(dout)\n\u001b[1;32m--> 114\u001b[0m dout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mbackward(dh)\n\u001b[0;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m dout\n",
      "File \u001b[1;32mc:\\Users\\Hi\\Desktop\\DeepLearningfromScratch\\src\\notebooks\\Part2\\../../modules/Part2\\attention_seq2seq.py:16\u001b[0m, in \u001b[0;36mAttentionEncoder.backward\u001b[1;34m(self, dhs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, dhs):\n\u001b[1;32m---> 16\u001b[0m     dout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm\u001b[39m.\u001b[39;49mbackward(dhs)\n\u001b[0;32m     17\u001b[0m     dout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed\u001b[39m.\u001b[39mbackward(dout)\n\u001b[0;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m dout\n",
      "File \u001b[1;32mc:\\Users\\Hi\\Desktop\\DeepLearningfromScratch\\src\\notebooks\\Part2\\../../modules/Part2\\common\\time_layers.py:211\u001b[0m, in \u001b[0;36mTimeLSTM.backward\u001b[1;34m(self, dhs)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(T)):\n\u001b[0;32m    210\u001b[0m     layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[t]\n\u001b[1;32m--> 211\u001b[0m     dx, dh, dc \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(dhs[:, t, :] \u001b[39m+\u001b[39;49m dh, dc)\n\u001b[0;32m    212\u001b[0m     dxs[:, t, :] \u001b[39m=\u001b[39m dx\n\u001b[0;32m    213\u001b[0m     \u001b[39mfor\u001b[39;00m i, grad \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(layer\u001b[39m.\u001b[39mgrads):\n",
      "File \u001b[1;32mc:\\Users\\Hi\\Desktop\\DeepLearningfromScratch\\src\\notebooks\\Part2\\../../modules/Part2\\common\\time_layers.py:163\u001b[0m, in \u001b[0;36mLSTM.backward\u001b[1;34m(self, dh_next, dc_next)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrads[\u001b[39m2\u001b[39m][\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m] \u001b[39m=\u001b[39m db\n\u001b[0;32m    162\u001b[0m dx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dA, Wx\u001b[39m.\u001b[39mT)\n\u001b[1;32m--> 163\u001b[0m dh_prev \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(dA, Wh\u001b[39m.\u001b[39;49mT)\n\u001b[0;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m dx, dh_prev, dc_prev\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "  trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "  \n",
    "  correct_num = 0\n",
    "  for i in range(len(x_test)):\n",
    "    question, correct = x_test[[i]], t_test[[i]]\n",
    "    verbose = i < 10\n",
    "    correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose, is_reverse=True)\n",
    "  acc = float(correct_num / len(x_test))\n",
    "  acc_list.append(acc)\n",
    "  \n",
    "  print(f\"검증 정확도 : {acc * 100 :.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과비교\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-25.png\" height=\"300\">\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-26.png\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.3 어텐션 시각화\n",
    "\n",
    "- 어텐션이 시계열 반환을 수행할 떄, 어느 원소에 주의를 기울이는지를 눈으로 살펴보려는 시도.\n",
    "- 각 시각의 어텐션 가중치를 인스턴스 변수로 보관하고 있으므로, 이를 시각화하기란 아주 간단\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-27.png\" height=\"300\">\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-28.png\" height=\"300\">\n",
    "\n",
    "- 어텐션은 인간이 이해할 수 있는 구조나 의미를 모델에 제공\n",
    "- 단어와 단어의 관련성 확인 가능\n",
    "- 모델의 처리 논리가 인간의 논리를 따르는지를 판단 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4 어텐션에 관한 남은 이야기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.1 양방향 RNN\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-29.png\" height=\"500\">\n",
    "\n",
    "- 왼쪽에 있는 정보만 담김\n",
    "- 주변정보를 균형있게 담고 싶다.\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-30.png\" height=\"500\">\n",
    "\n",
    "- 역방향으로 처리하는 LSTM 계층도 추가 \n",
    "- 두 LSTM 계층의 은닉 상태를 연결시킨 벡터를 최종 은닉 상태로 처리 (합하거나 평균내는 방법 등등)\n",
    "- 각 단어에 대응하는 은닉 상태 벡터에는 좌와 우 양쪽 방향으로부터의 정보를 집약할 수 있다.\n",
    "- 균형 잡힌 정보가 인코딩되게 된다 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.2 Attention 계층 사용 방법\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-31.png\" height=\"400\">\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-32.png\" height=\"400\">\n",
    "\n",
    "- 맥락벡터가 다음 시각의 LSTM 계층에 입력되도록 연결\n",
    "- 어떤 영향? -> 해보지 않으면 모른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.3 seq2seq 심층화와 skip 연결\n",
    "\n",
    "- 복잡한 문제 -> 높은 표현력을 요구 -> LSTM 계층을 깊게 쌓는다.\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-33.png\" height=\"400\">\n",
    "\n",
    "- Encoder와 Decoder에서는 같은 층수의 LSTM 계층을 이용하는 것이 일반적\n",
    "- Attention 계층의 사용법은 여러 변형이 존재\n",
    "  - Decoder의 LSTM 계층의 은닉 상태를 Attention 계층에 입력하고,    \n",
    "Attention 계층의 출력인 맥락벡터를 Decoder의 여러 계층(LSTM 계층과 Affine 계층)으로 전파\n",
    "- 층을 깊게 할 떄 사용되는 중요 기법 중 skip connection (residual connection, short-cut)\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-34.png\" height=\"400\">\n",
    "\n",
    "- 계층을 건너뛰는 연결. \n",
    "- skip 연결의 기울기가 아무런 영향을 받지 않고 모든 계층으로 흐름\n",
    "- 층이 깊어져도 기울기가 소실되지 않고 전파되어, 결과적으로 좋은 학습을 기대할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.5 어텐션 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1 구글 신경망 기계 번역 (GNMT)\n",
    "\n",
    "규칙기반 번역 -> 용례 기반 번역 -> 통계 기반 번역 -> 신경망 기계 번역 (NMT, Neural Machine Translation)\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-35.png\" height=\"400\">\n",
    "\n",
    "- LSTM 계층의 다층화\n",
    "- 양방향 LSTM\n",
    "- skip 연결\n",
    "- 다수의 GPU로 분산 학습\n",
    "- 낮은 빈도의 단어처리나 추론 고속화를 위한 양자화 등 다양한 연구가 이루어지고 있다.\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-36.png\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.2 트랜스포머\n",
    "\n",
    "- RNN은 이전 시각에 계산한 결과를 이용하여 순서대로 계산\n",
    "- RNN의 계산을 시간 방향으로 병렬 계산하기란 불가능\n",
    "- 이는 GPU를 사용한 병렬 계산 환경에서 이뤄진다는 점을 생각하면 큰 병목이다.\n",
    "- RNN을 없애는 연구가 활발히 이뤄지고 있다.\n",
    "- Tranformer model\n",
    "- self-attention\n",
    "- 하나의 시계열 데이터를 대상으로 한 어텐션으로,  \n",
    " 하나의 시게열 데이터 내에서 각 원소가 다른 원소들과 어떻게 관련되는지를 살펴보자는 취지\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-37.png\" height=\"200\">\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-38.png\" height=\"400\">\n",
    "\n",
    "- 두 입력선이 모두 하나의 시계열 데이터로부터 나온다. \n",
    "- 하나의 시계열 데이터 내에서의 원소간 대응 관계가 구해짐\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-39.png\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.3 뉴럴 튜링 머신 (NTM)\n",
    "\n",
    "- 신경망에도 외부 메모리를 이용하여 새로운 힘을 부여\n",
    "- 외부 메모리를 통한 확장\n",
    "- RNN, LSTM 내부 상태를 활용해 시계열 데이터를 기억.   \n",
    "내부 상태는 길이가 고정이라 채워 넣을 수 있는 정보량이 제한적\n",
    "- RNN 외부에 기억장치를 두고 필요한 정보를 거기에 적절하게 기록하는 방안을 착안\n",
    "\n",
    "- RNN의 외부에 정보 저장용 메모리 기능을 배치하고,    \n",
    "어텐션을 이용하여 그 메모리로부터 필요한 정보를 읽거나 쓰는 방법 \n",
    "\n",
    "- NTM ,DeepMind , DNC라는 기법으로 개선한 논문. NTM의 메모리 조작을 더욱 강화학 버전\n",
    "\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-40.png\" height=\"400\">\n",
    "<img src=\"../../../data/deep_learning_2_images/fig 8-41.png\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLFS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
